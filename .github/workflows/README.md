# Chopsticks GitHub Actions Workflows

This directory contains CI/CD workflows for automated testing and validation of the Chopsticks framework.

## Workflows

### 1. CI Pipeline (`ci.yml`)

**The main CI pipeline** that runs on all pull requests and pushes to main.

#### Jobs

1. **Lint** - Code quality checks
   - Runs `ruff check` for linting
   - Runs `ruff format --check` for code formatting
   - Fast execution (~1-2 minutes)

2. **Unit Tests** - Fast, isolated tests
   - Runs pytest on unit tests
   - Generates coverage report (XML format)
   - Uploads coverage as artifact (7-day retention)
   - Fast execution (~1-2 minutes)

**Triggers:**
- Pull requests to `main`
- Pushes to `main`

**Total Duration:** ~2-3 minutes

### 2. Lint Workflow (`lint.yml`)

Standalone linting workflow for quick code quality validation.

**What it does:**
- Runs `ruff check` to find code issues
- Runs `ruff format --check` to validate formatting

**When to use:** For quick validation before pushing.

### 3. Unit Test Workflow (`test.yml`)

Standalone unit testing workflow.

**What it does:**
- Runs pytest on unit tests
- Generates coverage reports (XML and terminal output)
- Uploads coverage artifacts (7-day retention)

**When to use:** For running tests independently of linting.

### 4. Functional Tests (`functional-tests.yml`)

Comprehensive integration tests with real MicroCeph cluster.

**What it tests:**
1. **MicroCeph Setup**: Single-node cluster with 3 OSDs
2. **S3 Configuration**: User creation and RGW endpoint setup
3. **S3 Operations**: 2-minute stress test with multiple operations
4. **Metrics Collection**: Validates metrics export
5. **Success Validation**: Ensures 100% success rate

**Duration:** ~15-20 minutes
**Artifacts:** Test reports and metrics (30-day retention)

## Running Tests Locally

### Lint

```bash
# Check code
uv run ruff check src/chopsticks/

# Check formatting
uv run ruff format --check src/chopsticks/

# Auto-fix issues
uv run ruff check --fix src/chopsticks/

# Auto-format code
uv run ruff format src/chopsticks/
```

### Unit Tests

```bash
# Run all unit tests
uv run pytest tests/unit/ -v

# Run with coverage
uv run pytest tests/unit/ --cov=src/chopsticks --cov-report=term

# Run specific test file
uv run pytest tests/unit/test_metrics.py -v

# Run specific test
uv run pytest tests/unit/test_metrics.py::TestOperationMetric::test_metric_creation -v
```

### Functional Tests

```bash
# Run with default settings (requires MicroCeph)
./scripts/run-functional-test.sh

# Custom configuration
TEST_DURATION=5m LARGE_OBJECT_SIZE=50 TEST_USERS=5 ./scripts/run-functional-test.sh
```

## CI Pipeline Flow

```
┌─────────────────────────────────────────┐
│         Pull Request Created            │
└─────────────────────────────────────────┘
                    │
                    ▼
      ┌─────────────────────────┐
      │   CI Pipeline Starts    │
      └─────────────────────────┘
                    │
        ┌───────────┴───────────┐
        │                       │
        ▼                       ▼
┌──────────────┐      ┌──────────────────┐
│     Lint     │      │   Unit Tests     │
│   (~2 min)   │      │    (~2 min)      │
└──────────────┘      └──────────────────┘
        │                       │
        │                       │
        │    Both run in        │
        │     parallel          │
        │                       │
        └───────────┬───────────┘
                    ▼
         ┌──────────────────┐
         │   All Passed ✅   │
         │  Ready to Merge   │
         └──────────────────┘
```

## Test Coverage

### Unit Tests (`tests/unit/`)

- **Metrics module**: OperationMetric, MetricsCollector, TestConfiguration
- **Configuration**: YAML loading and validation
- **Fast execution**: No external dependencies
- **Coverage reporting**: Line-by-line coverage metrics

### Functional Tests (workflow-based)

- **Full S3 workload**: Real MicroCeph cluster
- **Metrics collection**: End-to-end validation
- **Performance validation**: Throughput and success rates

## Artifacts

### Coverage Report (Unit Tests)
- **Generated by**: `test.yml` and `ci.yml`
- **Format**: XML (`coverage.xml`)
- **Retention**: 7 days
- **Contains**: Line and branch coverage data

### Functional Test Report
- **Generated by**: `functional-tests.yml`
- **Files**: HTML report, metrics (JSON/CSV/JSONL), Locust stats
- **Retention**: 30 days

## Validation Criteria

### Lint
- ✅ No ruff errors or warnings
- ✅ Code properly formatted (PEP 8 compliant)

### Unit Tests
- ✅ All tests pass
- ✅ Coverage report generated successfully

### Functional Tests
- ✅ MicroCeph cluster healthy
- ✅ S3 operations execute successfully
- ✅ Success rate = 100%
- ✅ Minimum 10 operations completed
- ✅ Metrics files generated and valid

## Troubleshooting

### Lint Failures

```bash
# View errors
uv run ruff check src/chopsticks/

# Auto-fix
uv run ruff check --fix src/chopsticks/

# Format code
uv run ruff format src/chopsticks/
```

### Unit Test Failures

```bash
# Run with verbose output
uv run pytest tests/unit/ -vv

# Run failing test only
uv run pytest tests/unit/test_metrics.py::TestMetricsCollector -vv

# Run with debug output
uv run pytest tests/unit/ -vv -s
```

### Functional Test Failures

Check:
1. MicroCeph installation logs
2. S3 endpoint accessibility
3. Metrics file generation
4. Test artifacts in Actions tab

## Adding New Tests

### Unit Tests

1. Create test file in `tests/unit/test_<module>.py`
2. Follow naming convention: `test_<function_name>`
3. Use pytest fixtures from `conftest.py`
4. Run locally before pushing

Example:
```python
def test_my_feature(sample_test_config):
    """Test my new feature."""
    collector = MetricsCollector(
        test_run_id="test-123",
        test_config=sample_test_config,
    )
    assert collector is not None
```

### Integration Tests

1. Create test in `tests/integration/`
2. Mark with `@pytest.mark.integration`
3. May require external dependencies

## Performance

### Expected Timings

- **Lint**: 1-2 minutes
- **Unit Tests**: 1-2 minutes
- **Functional Tests**: 15-20 minutes
- **Total CI (Lint + Unit)**: ~2-3 minutes

### Optimization Tips

- Lint and unit tests run in parallel
- Use `pytest -n auto` for parallel test execution (future)
- Functional tests run independently (can be skipped for quick validation)

## Future Enhancements

Planned improvements:
- [ ] Add integration tests for different drivers
- [ ] Add RBD workload functional tests
- [ ] Add performance regression detection
- [ ] Add code coverage thresholds (e.g., 80%)
- [ ] Add automated benchmarking
- [ ] Add security scanning (Dependabot, CodeQL)
- [ ] Add matrix testing (multiple Python versions)
